{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":157270322,"sourceType":"kernelVersion"}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/patrickstarrrr/dvs128gesture-snntorch?scriptVersionId=161758341\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"IMB DVS128 Gesture Dataset (http://research.ibm.com/dvsgesture/) contains event-based recordings of 11 gestures made by 29 subjects under 3 different lighting conditions. A series of 11 gestures was recorded for each subject. Each gesture lasts 6 seconds. This work uses a preprocessed version of the original dataset (https://tonic.readthedocs.io/), where recordings that originally contained multiple labels have already been cut into respective samples. Also temporal precision is reduced to ms.\n\nTo avoid downloading dataset every session this kernel uses output from another kernel ([https://www.kaggle.com/code/dlarionov/create-dvs128gesture-tonic-dataset](https://www.kaggle.com/code/dlarionov/create-dvs128gesture-tonic-dataset))\n\nThis is the first part of the DVS128 Gesture Dataset exploration. It contains solution using spiking neural network implemented with snntorch. The event trace for each gesture is divided into dense frames, which are sequentially fed into a convolutional network containing, instead of the classical activation functions LIF neurons that can take into account the temporal aspect.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport numpy.lib.recfunctions as rf","metadata":{"execution":{"iopub.status.busy":"2024-01-13T04:49:06.05574Z","iopub.execute_input":"2024-01-13T04:49:06.0567Z","iopub.status.idle":"2024-01-13T04:49:06.070842Z","shell.execute_reply.started":"2024-01-13T04:49:06.05666Z","shell.execute_reply":"2024-01-13T04:49:06.069867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# each sequence of gestures for each subject is divided into 11 .npy files, which are named according to the target labels. \nfile_name = '/kaggle/input/create-dvs128gesture-tonic-dataset/DVSGesture/ibmGestureTest/user26_led/9.npy'\n\n# each file contains a list of events (x-pos, y-pos, polarity, timestamp).\narr = np.load(file_name)\narr[:, 3] *= 1000  # convert from ms to us\ndtype = np.dtype([(\"x\", np.int16), (\"y\", np.int16), (\"p\", bool), (\"t\", np.int64)])\narr = rf.unstructured_to_structured(arr, dtype)\n\nprint(\"A single event:\", arr[0], \"as (x-pos, y-pos, polarity, timestamp).\")\n\n# np.savetxt(\"./DVSGesture/ibmGestureTest/user26_led/9.csv\", arr, delimiter=\",\") # save as csv","metadata":{"execution":{"iopub.status.busy":"2024-01-13T04:49:06.973414Z","iopub.execute_input":"2024-01-13T04:49:06.974139Z","iopub.status.idle":"2024-01-13T04:49:07.161927Z","shell.execute_reply.started":"2024-01-13T04:49:06.9741Z","shell.execute_reply":"2024-01-13T04:49:07.160938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tonic --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-13T04:49:54.540323Z","iopub.execute_input":"2024-01-13T04:49:54.541201Z","iopub.status.idle":"2024-01-13T04:50:09.290787Z","shell.execute_reply.started":"2024-01-13T04:49:54.541171Z","shell.execute_reply":"2024-01-13T04:50:09.289687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tonic\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML\n\ndef to_frames(events):\n     # creates dense frames from events by binning them in different ways\n    frame_transform = tonic.transforms.ToFrame(\n        sensor_size=tonic.datasets.DVSGesture.sensor_size, \n        #time_window=10000)\n        n_time_bins=100)\n        #event_count=1000)\n    return frame_transform(events)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T04:50:09.292956Z","iopub.execute_input":"2024-01-13T04:50:09.293756Z","iopub.status.idle":"2024-01-13T04:50:25.629241Z","shell.execute_reply.started":"2024-01-13T04:50:09.293716Z","shell.execute_reply":"2024-01-13T04:50:25.628126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_path = '/kaggle/input/create-dvs128gesture-tonic-dataset'\ntrain = tonic.datasets.DVSGesture(save_to=dataset_path, train=True)\ntest = tonic.datasets.DVSGesture(save_to=dataset_path, train=False)\n\nevents, label = train[0]\nframes = to_frames(events)\n\nprint(\"Train dataset contains\", len(train), \"samples.\")\nprint(\"There are\", len(events), \"events in the selected sample.\")\nprint(\"A single event:\", events[0], \"as (x-pos, y-pos, polarity, timestamp).\")\nprint (frames.shape, label)\n\nani = tonic.utils.plot_animation(frames) # plot one frame\nHTML(ani.to_jshtml()) # animate all frames","metadata":{"execution":{"iopub.status.busy":"2024-01-13T04:50:29.427371Z","iopub.execute_input":"2024-01-13T04:50:29.428306Z","iopub.status.idle":"2024-01-13T04:50:33.413408Z","shell.execute_reply.started":"2024-01-13T04:50:29.428268Z","shell.execute_reply":"2024-01-13T04:50:33.412476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w,h=32,32\nn_frames=32 #100\ndebug = False\n\ntransforms = tonic.transforms.Compose([\n    tonic.transforms.Denoise(filter_time=10000), # removes outlier events with inactive surrounding pixels for 10ms\n    tonic.transforms.Downsample(sensor_size=tonic.datasets.DVSGesture.sensor_size, target_size=(w,h)), # downsampling image\n    tonic.transforms.ToFrame(sensor_size=(w,h,2), n_time_bins=n_frames), # n_frames frames per trail\n])\n\ntrain2 = tonic.datasets.DVSGesture(save_to=dataset_path, transform=transforms, train=True)\ntest2 = tonic.datasets.DVSGesture(save_to=dataset_path, transform=transforms, train=False)\n\ncached_train = train2 if debug else tonic.DiskCachedDataset(train2, cache_path='/temp/dvsgesture/train')\ncached_test = test2 if debug else tonic.DiskCachedDataset(test2, cache_path='/temp/dvsgesture/test')\n\nframes, label = cached_train[1]\nani = tonic.utils.plot_animation(frames)\nprint(frames.shape, label)\nHTML(ani.to_jshtml())","metadata":{"execution":{"iopub.status.busy":"2024-01-13T04:51:37.171679Z","iopub.execute_input":"2024-01-13T04:51:37.172628Z","iopub.status.idle":"2024-01-13T04:51:43.809757Z","shell.execute_reply.started":"2024-01-13T04:51:37.172571Z","shell.execute_reply":"2024-01-13T04:51:43.808785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install snntorch --quiet","metadata":{"execution":{"iopub.status.busy":"2024-01-13T04:51:58.666275Z","iopub.execute_input":"2024-01-13T04:51:58.667123Z","iopub.status.idle":"2024-01-13T04:52:11.222019Z","shell.execute_reply.started":"2024-01-13T04:51:58.667075Z","shell.execute_reply":"2024-01-13T04:52:11.220689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport snntorch as snn\nfrom snntorch import surrogate\nfrom snntorch import functional as SF\nfrom snntorch import utils","metadata":{"execution":{"iopub.status.busy":"2024-01-13T04:52:11.569203Z","iopub.execute_input":"2024-01-13T04:52:11.570114Z","iopub.status.idle":"2024-01-13T04:52:11.589062Z","shell.execute_reply.started":"2024-01-13T04:52:11.570078Z","shell.execute_reply":"2024-01-13T04:52:11.588044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-01-13T04:52:11.85595Z","iopub.execute_input":"2024-01-13T04:52:11.856615Z","iopub.status.idle":"2024-01-13T04:52:11.907862Z","shell.execute_reply.started":"2024-01-13T04:52:11.856568Z","shell.execute_reply":"2024-01-13T04:52:11.906846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"grad = snn.surrogate.fast_sigmoid(slope=25) # surrogate.atan()\nbeta = 0.5\n\n# 12C5-MP2-32C5-MP2-800FC11 https://snntorch.readthedocs.io/en/latest/tutorials/tutorial_7.html\nnet = nn.Sequential(\n    nn.Conv2d(2, 12, 5), # in_channels, out_channels, kernel_size\n    nn.MaxPool2d(2),\n    snn.Leaky(beta=beta, spike_grad=grad, init_hidden=True),\n    nn.Conv2d(12, 32, 5),\n    nn.MaxPool2d(2),\n    snn.Leaky(beta=beta, spike_grad=grad, init_hidden=True),\n    nn.Flatten(),\n    nn.Linear(800, 11), #800\n    snn.Leaky(beta=beta, spike_grad=grad, init_hidden=True, output=True)\n).to(device)\n\ndef forward_pass(net, data):\n    spk_rec = []\n    snn.utils.reset(net)  # resets hidden states for all LIF neurons in net\n    for step in range(data.size(0)): # data.size(0) = number of time steps\n        spk_out, mem_out = net(data[step])\n        spk_rec.append(spk_out)\n    return torch.stack(spk_rec)\n\noptimizer = torch.optim.Adam(net.parameters(), lr=0.002, betas=(0.9, 0.999))\nloss_fn = SF.mse_count_loss(correct_rate=0.8, incorrect_rate=0.2)\n\nloss_hist = []\nacc_hist = []\ntest_acc_hist = []","metadata":{"execution":{"iopub.status.busy":"2024-01-13T04:53:13.329458Z","iopub.execute_input":"2024-01-13T04:53:13.330391Z","iopub.status.idle":"2024-01-13T04:53:13.55915Z","shell.execute_reply.started":"2024-01-13T04:53:13.330355Z","shell.execute_reply":"2024-01-13T04:53:13.55834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate_model():\n    correct, total = 0, 0  \n    for batch, (data, targets) in enumerate(iter(test_loader)): \n        data, targets = data.to(device), targets.to(device) # [n_frames, batch, polarity, x-pos, y-pos] [batch] \n        spk_rec = forward_pass(net, data)         \n        correct += SF.accuracy_rate(spk_rec, targets) * data.shape[0]\n        total += data.shape[0]\n    return correct/total","metadata":{"execution":{"iopub.status.busy":"2024-01-13T05:03:29.33617Z","iopub.execute_input":"2024-01-13T05:03:29.33694Z","iopub.status.idle":"2024-01-13T05:03:29.342709Z","shell.execute_reply.started":"2024-01-13T05:03:29.336907Z","shell.execute_reply":"2024-01-13T05:03:29.341737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 100\ncnt = 0\n\ntrain_loader = torch.utils.data.DataLoader(cached_train, batch_size=64, shuffle=True, drop_last=True, \n                                           collate_fn=tonic.collation.PadTensors(batch_first=False))\ntest_loader = torch.utils.data.DataLoader(cached_test, batch_size=32, shuffle=True, drop_last=True, \n                                          collate_fn=tonic.collation.PadTensors(batch_first=False))\n\nfor epoch in range(num_epochs):\n    for batch, (data, targets) in enumerate(iter(train_loader)):\n        data = data.to(device)\n        targets = targets.to(device)\n\n        net.train()\n        # propagating one batch through the network and evaluating loss\n        spk_rec = forward_pass(net, data)\n        loss = loss_fn(spk_rec, targets)\n\n        # Gradient calculation + weight update\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Store loss history for future plotting\n        loss_hist.append(loss.item())\n\n        acc = SF.accuracy_rate(spk_rec, targets)\n        acc_hist.append(acc)\n\n        if cnt % 50 == 0:\n            print(f\"Epoch {epoch}, Iteration {batch} \\nTrain Loss: {loss.item():.2f}\")\n            print(f\"Train Accuracy: {acc * 100:.2f}%\")\n            test_acc = validate_model()            \n            test_acc_hist.append(test_acc)\n            print(f\"Test Accuracy: {test_acc * 100:.2f}%\\n\")\n\n        cnt+=1","metadata":{"execution":{"iopub.status.busy":"2024-01-13T05:09:56.81614Z","iopub.execute_input":"2024-01-13T05:09:56.817104Z","iopub.status.idle":"2024-01-13T05:50:37.798404Z","shell.execute_reply.started":"2024-01-13T05:09:56.81707Z","shell.execute_reply":"2024-01-13T05:50:37.797444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(18,4))\n\n# Plot Train Accuracy\naxes[0].plot(acc_hist)\naxes[0].set_title(\"Train Set Accuracy\")\naxes[0].set_xlabel(\"Iteration\")\naxes[0].set_ylabel(\"Accuracy\")\n\n# Plot Test Accuracy\naxes[1].plot(test_acc_hist)\naxes[1].set_title(\"Test Set Accuracy\")\naxes[1].set_xlabel(\"Iteration\")\naxes[1].set_ylabel(\"Accuracy\")\n\n# Plot Training Loss\naxes[2].plot(loss_hist)\naxes[2].set_title(\"Loss History\")\naxes[2].set_xlabel(\"Iteration\")\naxes[2].set_ylabel(\"Loss\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-13T05:50:50.880889Z","iopub.execute_input":"2024-01-13T05:50:50.881793Z","iopub.status.idle":"2024-01-13T05:50:51.585012Z","shell.execute_reply.started":"2024-01-13T05:50:50.881756Z","shell.execute_reply":"2024-01-13T05:50:51.584083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validate_model(), np.max(test_acc_hist)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T05:50:51.586511Z","iopub.execute_input":"2024-01-13T05:50:51.586838Z","iopub.status.idle":"2024-01-13T05:50:52.38951Z","shell.execute_reply.started":"2024-01-13T05:50:51.586811Z","shell.execute_reply":"2024-01-13T05:50:52.388624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}